{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               essay\n",
       "0  Do u believe there are books, music, magizines...\n",
       "1  I strongly believe that there are some materia...\n",
       "2  Do you think that certain books, movies, magaz...\n",
       "3  Censorship in libraries should definetly be al...\n",
       "4  Many books are helpful as you @MONTH1 know by ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do u believe there are books, music, magizines...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I strongly believe that there are some materia...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Do you think that certain books, movies, magaz...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Censorship in libraries should definetly be al...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Many books are helpful as you @MONTH1 know by ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 331
    }
   ],
   "source": [
    "essays = pd.read_csv('essays_for_mlm_tuning.csv')\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = essays.values.tolist()\n",
    "text = list(map(lambda x: x[0], text))"
   ]
  },
  {
   "source": [
    "Check if all tokens in the essay texts are accepted by the model. If not, extend the model's vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokens in essay texts: 24112\n",
      "missing_tokens: 17931\n"
     ]
    }
   ],
   "source": [
    "# Save initial embeddings to check if changed later\n",
    "initial_embeddings = model.distilbert.embeddings.word_embeddings\n",
    "\n",
    "# Save vocabulary to compare with the essay vocabulary\n",
    "tokenizer.save_vocabulary('vocab.txt')\n",
    "\n",
    "essay_tokens = []\n",
    "\n",
    "for t in text:\n",
    "    tokens = t.split()\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in essay_tokens:\n",
    "            essay_tokens.append(token)\n",
    "\n",
    "essay_tokens.sort()\n",
    "print(f'tokens in essay texts: {len(essay_tokens)}')\n",
    "\n",
    "missing_tokens = []\n",
    "\n",
    "with open('vocab.txt', 'r') as vocab:\n",
    "    model_tokens = vocab.readlines()\n",
    "    model_tokens = list(map(lambda x: x[:-1], model_tokens))\n",
    "\n",
    "    for token in essay_tokens:\n",
    "        if token not in model_tokens:\n",
    "            missing_tokens.append(token)\n",
    "\n",
    "print(f\"missing_tokens: {len(missing_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'should good books be taken off the shelf because their not appropriate for should libraries be only child censorship in public libraries the answer to solve the problem with children in on what books they want to public libraries have the title for a taking good books off that not be good books for instead should be in sections for and i that censorship of libraries would be a very ignorant not to mention that it would create an in the public tax i tax would appreciate that their tax money going toward things that actually a lot of the public like those books and other media sources being taken some find the books but that mean that they have to get rid of the when they can just put the book i do that everyone should have a say in their public libraries to improve not children should be restricted from the sections that are not appropriate for their age and should be to locations of sections that are appropriate for their censorship on this subject means sealing other ideas off and away from the public some of those ideas can educate people very well with quality knowledge or but are brain candy and just are fun to if people like paterson are concerned about what readers can read and they should be what their children are reading and allow them or not the the libraries just carry the books that are offensive show the reader new and interesting some for example like of show what life was like in that and even if the book has offensive language for children means that it should be correctly'"
      ]
     },
     "metadata": {},
     "execution_count": 334
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    # Naive approach: just remove whatever token is missing (for now)\n",
    "    tokens = list(filter(lambda token: True if token not in missing_tokens else False, tokens))\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "text = list(map(lambda txt: clean_text(txt), text))\n",
    "text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'do u believe there are and movies in are these could consist of nude pictures and so most parents do not want to see there kids getting a hold of this type of you maybe think could we do about this or least come to a to make almost everyone i have so reasons why we should put this type of material away from kids sight and first of the books and that have any nude in should have their own section be i know this might be of work but it is so kids aloud to go in this will help what little kids there are also books and that are offensive to children of a different these books should be removed from the shelves also because they peoples and this might make customers leave your i am not saying to take ever single book of your but a least reduce the number of books of your out of kids eyes mostly and some adults if they are next thing that comes to mind is the music on or which ever you in this day and age there are of musicians out there that have of bad words in their these are songs parents like there children to listen because parents are afraid of their children here these words and repeating them at these type of should be taken of shelves where kids and put in a section also adults i know this stop kids from hearing these words and say but at least the parents can blame you for their child saying these because you did the smart thing and put these out of kids there are songs that are to people but i know you go every song and take them off the but a least take the off the that you know are offensive to the last that comes to mind that should be taken off the shelves that are material children see is the there are movies out there that are to to and to much that kids need to these type of movies need to also get a section that is for adults then you have the movies that need to just be taking off the because they are to offensive to some i know that you say but if it is offensive to they have to get this maybe true but a least move the to a section that those people that get offended those were my thoughts on what should be done with material to offensive and not for eyes or might be a little extreme but i hope you at least take some of these things into you please all but you can thanks for reading this and i hope it as changed your thoughts on material bad for kids to see'"
      ]
     },
     "metadata": {},
     "execution_count": 335
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2079, 1057,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 6118,  ...,    0,    0,    0],\n",
       "        [ 101, 2079, 2017,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1045, 2228,  ...,    0,    0,    0],\n",
       "        [ 101, 2087, 2808,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2228,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "execution_count": 336
    }
   ],
   "source": [
    "# From the dataset manipulation notebook, the max length is 3241\n",
    "# Bert expects 512 though\n",
    "inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "metadata": {},
     "execution_count": 337
    }
   ],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 338
    }
   ],
   "source": [
    "MASK_PROBABILITY = 0.1\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "\n",
    "# create mask array\n",
    "# NOTE tokens 101 and 102 ar special (CLS and SEP), and 0 is a padding so we don't mask them\n",
    "mask_arr = (rand < MASK_PROBABILITY) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[25,\n",
       " 27,\n",
       " 41,\n",
       " 47,\n",
       " 84,\n",
       " 92,\n",
       " 97,\n",
       " 99,\n",
       " 100,\n",
       " 109,\n",
       " 110,\n",
       " 130,\n",
       " 157,\n",
       " 168,\n",
       " 203,\n",
       " 209,\n",
       " 232,\n",
       " 241,\n",
       " 252,\n",
       " 253,\n",
       " 275,\n",
       " 304,\n",
       " 323,\n",
       " 325,\n",
       " 329,\n",
       " 330,\n",
       " 340,\n",
       " 353,\n",
       " 356,\n",
       " 370,\n",
       " 404,\n",
       " 410,\n",
       " 419,\n",
       " 434,\n",
       " 440,\n",
       " 447,\n",
       " 453]"
      ]
     },
     "metadata": {},
     "execution_count": 339
    }
   ],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "\n",
    "selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 101, 2079, 1057,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 6118,  ...,    0,    0,    0],\n",
       "        [ 101, 2079, 2017,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1045, 2228,  ...,    0,    0,    0],\n",
       "        [ 101, 2087, 2808,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2228,  ...,    0,    0,    0]])"
      ]
     },
     "metadata": {},
     "execution_count": 340
    }
   ],
   "source": [
    "# Apply masks (token 103) where the random number was below the probability\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = tokenizer.mask_token_id\n",
    "\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset to feed the model\n",
    "class AESDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return { \n",
    "            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "dataset = AESDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataloader used during training\n",
    "BATCH_SIZE = 64\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using: cpu\n"
     ]
    }
   ],
   "source": [
    "# Enable CUDA if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "print('using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate training mode for the model\n",
    "from transformers import AdamW # Using Weighted Adam optimizer\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]<ipython-input-341-626a67d1eeb4>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
      "Epoch 0:   3%|▎         | 1/34 [09:44<5:21:44, 584.98s/it, loss=7.05]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-e16c8d8bbd97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# calculate loss for every parameter that needs grad update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "from tqdm import tqdm  # tqdm provides a progress bar for training\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
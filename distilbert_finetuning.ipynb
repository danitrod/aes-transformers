{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               essay\n",
       "0  9gag - number of posts by 9gag user  why 9gag?...\n",
       "1  social media spring 2013 project 1 - mining th...\n",
       "2  social media: individual work the long tail of...\n",
       "3  youtube user information analysis march 18, 20...\n",
       "4  social medias report on long tail effect intro..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9gag - number of posts by 9gag user  why 9gag?...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>social media spring 2013 project 1 - mining th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>social media: individual work the long tail of...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>youtube user information analysis march 18, 20...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>social medias report on long tail effect intro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "essays = pd.read_csv('swiss.csv')\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = essays.values.tolist()\n",
    "text = list(map(lambda x: x[0], text))"
   ]
  },
  {
   "source": [
    "Check if all tokens in the essay texts are accepted by the model. If not, extend the model's vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokens in essay texts: 5467\n",
      "missing_tokens: 2854\n"
     ]
    }
   ],
   "source": [
    "# Save initial embeddings to check if changed later\n",
    "initial_embeddings = model.distilbert.embeddings.word_embeddings\n",
    "\n",
    "# Save vocabulary to compare with the essay vocabulary\n",
    "tokenizer.save_vocabulary('vocab.txt')\n",
    "\n",
    "essay_tokens = []\n",
    "\n",
    "for t in text:\n",
    "    tokens = t.split()\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in essay_tokens:\n",
    "            essay_tokens.append(token)\n",
    "\n",
    "essay_tokens.sort()\n",
    "print(f'tokens in essay texts: {len(essay_tokens)}')\n",
    "\n",
    "missing_tokens = []\n",
    "\n",
    "with open('vocab.txt', 'r') as vocab:\n",
    "    model_tokens = vocab.readlines()\n",
    "    model_tokens = list(map(lambda x: x[:-1], model_tokens))\n",
    "\n",
    "    for token in essay_tokens:\n",
    "        if token not in model_tokens:\n",
    "            missing_tokens.append(token)\n",
    "\n",
    "print(f\"missing_tokens: {len(missing_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'project i mining the social web site chosen is is a chinese website for girls and women to share their experiences on provides most kinds of cosmetic appeared in market and cosmetics are divided in as the figure this cosmetic distribution will attract large numbers of customers to search for general as well as specific comments about this approach attracts the site visitors at all steps in finding and sharing comments about their interest for when visitors want to search for women skin care they can also search for specific types as facial or eye the same with sharing their comments on specific traffic from more specific search generally containing more than 3 is commonly referred to as long tail this traffic is highly desirable due to the increasing amount of long tail searches by consumers and high conversion rates commonly associated with these the success of the effective site is typically measured by the increase in daily reach and daily traffic rank trend the following illustrates an increase in daily reach during the year 2012 when first category its products comments in a the business listed below experienced a growth in daily reach during last 3 months and a 22 ranking growth in daily traffic rank figure daily reach of 2 years data of daily traffic rank trend of 2 years data of this site will experience some challenges because not all customers know exactly about the product names and their sometimes they just want to find something to the hypothesis will not work for this kind of'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    # Naive approach: just remove whatever token is missing (for now)\n",
    "    tokens = list(filter(lambda token: True if token not in missing_tokens else False, tokens))\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "text = list(map(lambda txt: clean_text(txt), text))\n",
    "text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'- number of posts by user why for this i chose a social media platform launched in 2008 with a similar goal as share images that users all over the world the community of users then vote and can share the the website became very and had for instance 1 billion monthly page views as of december a lot of students use this website as a casual and i felt it would be interesting to have some statistics about the long tail hypothesis is one of those website with a strong feeling of community and explain briefly how it there are three steps to the life of an image on when you an it gets on the vote everybody can vote for the image they to there are a lot of pictures of the vote but if it gets enough the image will be promoted to the once the procedure is and if the image became successful it will go to the most popular the hot which is the of one of the main goal for the people who post there is to get on the hot who could be see as a fantastic reward with almost a mystical feeling for certain our hypothesis is that while there must be some users who are deeply rooted into this website and countless the vast majority should post only a few results for this i crawled the with the help of similar pattern in the display of images and user i was able to gather data about the number of posts by the results are display on the next we can see that the number of post by user indeed respect the long tail while some have a large number of the vast majority only post this shows that not everyone tries again and again to get to the hot and one contents himself with quite a few posts before getting the ones who post a lot of post are really'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1011, 2193,  ...,    0,    0,    0],\n",
       "        [ 101, 2591, 2865,  ..., 3643, 1997,  102],\n",
       "        [ 101, 2591, 3265,  ..., 2003, 3491,  102],\n",
       "        ...,\n",
       "        [ 101, 1996, 2146,  ..., 2012, 2107,  102],\n",
       "        [ 101, 2591, 2865,  ..., 2145, 3961,  102],\n",
       "        [ 101, 1011, 2622,  ..., 2312, 9785,  102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# From the dataset manipulation notebook, the max length is 3241\n",
    "# Bert expects 512 though\n",
    "inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "MASK_PROBABILITY = 0.1\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "\n",
    "# create mask array\n",
    "# NOTE tokens 101 and 102 ar special (CLS and SEP), and 0 is a padding so we don't mask them\n",
    "mask_arr = (rand < MASK_PROBABILITY) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 11,\n",
       " 25,\n",
       " 38,\n",
       " 53,\n",
       " 64,\n",
       " 94,\n",
       " 97,\n",
       " 98,\n",
       " 100,\n",
       " 104,\n",
       " 111,\n",
       " 121,\n",
       " 127,\n",
       " 130,\n",
       " 141,\n",
       " 146,\n",
       " 155,\n",
       " 171,\n",
       " 178,\n",
       " 197,\n",
       " 198,\n",
       " 207,\n",
       " 216,\n",
       " 223,\n",
       " 227,\n",
       " 235,\n",
       " 250,\n",
       " 265,\n",
       " 271,\n",
       " 273,\n",
       " 281,\n",
       " 291,\n",
       " 299,\n",
       " 305,\n",
       " 312,\n",
       " 314,\n",
       " 316]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "\n",
    "selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 101,  103,  103,  ...,    0,    0,    0],\n",
       "        [ 101, 2591,  103,  ..., 3643, 1997,  102],\n",
       "        [ 101, 2591, 3265,  ..., 2003, 3491,  102],\n",
       "        ...,\n",
       "        [ 101, 1996, 2146,  ..., 2012, 2107,  102],\n",
       "        [ 101,  103, 2865,  ..., 2145, 3961,  102],\n",
       "        [ 101, 1011, 2622,  ..., 2312, 9785,  102]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Apply masks (token 103) where the random number was below the probability\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = tokenizer.mask_token_id\n",
    "\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset to feed the model\n",
    "class AESDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return { \n",
    "            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "dataset = AESDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataloader used during training\n",
    "BATCH_SIZE = 64\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using: cpu\n"
     ]
    }
   ],
   "source": [
    "# Enable CUDA if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "print('using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate training mode for the model\n",
    "from transformers import AdamW # Using Weighted Adam optimizer\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-21-626a67d1eeb4>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
      "  0%|          | 0/1 [03:35<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "from tqdm import tqdm  # tqdm provides a progress bar for training\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}